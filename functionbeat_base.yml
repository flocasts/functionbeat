cloud:
  auth: "${CLOUD_AUTH}"
  id: "${CLOUD_ID}"
keystore:
  path: './beats.keystore'
logging:
  json: true
  level: info
  to_files: false
  to_stderr: true
output:
  elasticsearch:
    bulk_max_size: 50
    enabled: true
    pipeline: cloudwatch-logs

path.logs: /tmp/logs

processors:
- add_tags:
    tags: [VAR_ENV]
    target: log.env
- drop_fields:
    fields:
    - agent.ephemeral_id
    - agent.hostname
    - agent.id
    - agent.type
    - agent.version
    - ecs.version
    - host.name
    - id
    - log_group
    - log_stream
    - owner
    - subscription_filters
    - message_type
- dissect:
    field: message
    target_prefix: dissect
    tokenizer: "%{timestamp}\t%{id}\t%{msg}"
- rename:
    fields:
    - from: "dissect.msg"
      to: "log.message"
    - from: "dissect.timestamp"
      to: "log.timestamp"
    when:
      contains:
        dissect.msg: 'Task timed out'
- decode_json_fields:
    fields:
    - dissect.msg
    max_depth: 5
    process_array: true
    target: log
    when:
      not:
        contains:
          dissect.msg: 'Task timed out'
- drop_fields:
    fields:
    - dissect
    - message

queue:
  mem:
    events: 128
    flush:
      min_events: 8
      timeout: 10s
setup:
  ilm:
    enabled: auto
  kibana:
    host: 4f95dc93e90e41f881de33d25141f8ac.us-west-2.aws.found.io:9243
    username: "${KIBANA_USER}"
    password: "${KIBANA_PASS}"
    protocol: https

  template:
    enabled: true
functionbeat:
  provider:
    aws:
      deploy_bucket: flosports-functionbeat-deploy
      functions:
      - type: cloudwatch_logs
        concurrency: 20
        description: lambda function for cloudwatch logs
        enabled: true
        memory_size: 1024MiB
        name: VAR_NAME
        triggers:
